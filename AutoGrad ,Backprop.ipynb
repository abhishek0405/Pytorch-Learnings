{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2716,  1.2864,  1.3919], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad = True) #specify this, now all operations over x is tracked for computational graph.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7284, 3.2864, 3.3919], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x+2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.9874, 10.8004, 11.5050], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6930, -0.0299,  0.9313])\n"
     ]
    }
   ],
   "source": [
    "z = z.mean()#gradient only for scalar output\n",
    "z.backward() #dz/dxz \n",
    "print(x.grad) #can be called only after z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### z.backward(t) if t is vector of same size of z. But usually z would be scalar in the end so not used much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.8817, 2.0288, 5.4211])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(3)\n",
    "z.backward(t)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preventing pytorch from tracking all grads, 3 ways:\n",
    "1. x.requires_grad_(False)\n",
    "2. y =x.detach -> Same values but no grad tracked\n",
    "3. with torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6967, 0.9514, 0.9182, 0.9564], requires_grad=True)\n",
      "tensor([0.6967, 0.9514, 0.9182, 0.9564])\n"
     ]
    }
   ],
   "source": [
    "#method 1\n",
    "x = torch.rand(4,requires_grad=True)\n",
    "print(x)\n",
    "x.requires_grad_(False)#inplace\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2919, 0.2432, 0.4247, 0.1595], requires_grad=True)\n",
      "tensor([0.2919, 0.2432, 0.4247, 0.1595])\n"
     ]
    }
   ],
   "source": [
    "#method 2\n",
    "x = torch.rand(4,requires_grad=True)\n",
    "print(x)\n",
    "y = x.detach()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.4446, 5.4050, 4.2906])\n",
      "tensor([5.4446, 5.4050, 4.2906], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "x = torch.rand(3,requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    #no grad calculated for any\n",
    "    y=x+2\n",
    "    z=y*y\n",
    "    print(z)\n",
    "y1=x+2\n",
    "z1=y1*y1\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5.])\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# we need to reset gradients very important, usually needed for optimizers. ie optim.step() or weight update then optim.zero_grad_()\n",
    "weights = torch.ones(4,requires_grad=True)\n",
    "for epoch in range(5):\n",
    "    model_op = (weights*5).sum()\n",
    "    model_op.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_()#gradients are stacked hence we need to reset after every iteration else incorrect results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop-> aim is to get gradient of loss with respect to input param and then apply optimisation technique.The computational graph tracks local gradients which are then used later during chain rule.(It tracks grad of current op wrt prev input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "w = torch.tensor(1.0,requires_grad=True)\n",
    "#fwd pass\n",
    "y_hat = w*x\n",
    "loss = (y_hat-y)**2\n",
    "print(loss)\n",
    "#loss will be (1-2)**2 = 1\n",
    "\n",
    "#backward pass\n",
    "loss.backward()\n",
    "#now all gradients are calculated\n",
    "#dloss/dw = dloss/dy * dy/dw \n",
    "print(w.grad)\n",
    "#now weight update acc to gradients\n",
    "#clear gradients else wrong computation.\n",
    "#repeat in a loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
